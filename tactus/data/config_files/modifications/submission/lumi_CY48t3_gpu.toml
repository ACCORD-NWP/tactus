[submission]
  lfftw = false
  nfpgrib = 141
  nproma = -256

[submission.task_exceptions.Forecast]
  bindir = "/pfs/lustrep4/projappl/project_465000527/dhaumont/accord/pack/freeze_gpu_feb_2024/bin"

[submission.task_exceptions.Forecast.ENV]

[submission.types.Forecast_cpu]
  tasks = []

[submission.types.Forecast_gpu]
  NPROC = 16
  SCHOST = "lumi-batch"
  tasks = ["Forecast"]
  WRAPPER = "ulimit -s unlimited; srun -n @NPROC@ ./select_gpu"

[submission.types.Forecast_gpu.BATCH]
  CPU = "#SBATCH --cpus-per-task=6"
  GPUS_PER_NODE = "#SBATCH --gpus-per-node=8"
  NAME = "#SBATCH --job-name=@TASK_NAME@"
  NODES = "#SBATCH --nodes=2"
  NTASKS = "#SBATCH --ntasks=16"
  PARTITION = "#SBATCH --partition=dev-g"
  TASKS_PER_NODE = "#SBATCH --ntasks-per-node=8"
  WALLSIGNAL = "#SBATCH --signal=USR1@30"
  WALLTIME = "#SBATCH --time=00:30:00"

[submission.types.Forecast_gpu.ENV]
  ECF_SSL = 1
  # Nodes, MPI tasks etc
  MPI_TASKS = 32      # $SLURM_NTASKS
  OMP_NUM_THREADS = 4 # CPUS_PER_TASK

  # Open-MP business :
  # OMP_PLACES looks important for the binding by srun :
  KMP_MONITOR_STACKSIZE = "4G"
  KMP_STACKSIZE = "4G"
  OMP_PLACES = "threads"
  OMP_STACKSIZE = "4G"

  # DR_HOOK
  DR_HOOK = 1
  DR_HOOK_IGNORE_SIGNALS = 8
  DR_HOOK_SHOW_PROCESS_OPTIONS = 0
  DR_HOOK_SILENT = 1

  # EC PROFILE
  EC_MEMINFO = 0
  EC_MPI_ATEXIT = 0
  EC_PROFILE_HEAP = 0
  EC_PROFILE_MEM = 0

  FI_CXI_RX_MATCH_MODE = "hybrid"

  #Environment variables for ALARO PARALLEL
  APL_ALARO = 1
  INPART = 1
  PARALLEL = 1

  #Environment for LUMI GPU
  #Allocate 8Gb of heap size on the GPU
  CRAY_ACC_MALLOC_HEAPSIZE = 8000000000

  #Allow direct GPU to GPU MPI connection
  MPICH_GPU_SUPPORT_ENABLED = 1

  # GPU: bind 8 MPI tasks to certain CPU cores (so that GPUs 0-7 are connected to tasks 0-7)
  CPU_BIND = "mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000"

  #AMD_LOG_LEVEL=3
  #CRAY_ACC_DEBUG=3
  #ACC_NOTIFY=3
  NOUTPUT = 2

[submission.types.Forecast_gpu.MODULES]
  00_PURGE = ["--force", "purge"]
  01_LUMI = ["load", "LUMI/23.03"]
  02_PARTITION_G = ["load", "partition/G"]
  03_CPE = ["load", "cpe/23.03"]
  04_PRGENV_CRAY = ["load", "PrgEnv-cray"]
  05_TRENTO = ["load", "craype-x86-trento"]
  06_ACCEL = ["load", "craype-accel-amd-gfx90a"]
  07_ROCM = ["load", "rocm"]
  08_MPICH = ["load", "cray-mpich"]
  09_LIBSCI = ["load", "cray-libsci"]
  10_FFTW = ["load", "cray-fftw"]
  11_PYTHON = ["load", "cray-python/3.10.10"]
  12_HDF5 = ["load", " cray-hdf5-parallel"]
  13_NETCDF = ["load", "cray-netcdf-hdf5parallel"]
  14_BUILDTOOLS = ["load", "buildtools"]
  15_MODULE_PATH = ["use", "/scratch/project_465000527/jasinskas/scl/modules"]
  16_ECFLOW = ["load", "scl-ecflow_23"]

  #  RTTOV_COEFDIR = "/scratch/project_465000527/de_330_common_data/DEODE/satellite/rtcoef_rttov12/harm_coef/"
