#CPU
#bindir = "/users/ospaniel/pack/48t3_main.06.CRAYFTN1501.x/bin"

#DAAN OLD ROOTPACK
#bindir = "/pfs/lustrep4/projappl/project_465000527/dadegrau/accord/rootpack/48t3_main.01.CRAYFTN1501.x/bin"

#DAAN NEW ROOTPACK (require to set LFFTW = .FALSE.)
#bindir = "/project/project_465000527/dadegrau/accord/rootpack/48t3_main.01.CRAYFTN1501_ECTRANS.y/bin"

#DAAN GPU
bindir = "/pfs/lustrep4/projappl/project_465000527/dadegrau/accord/pack/alaro_gpu/bin"



submit_types = ["background_vm", "background_hpc", "serial", "parallel"]
default_submit_type = "parallel"

[task]
wrapper = "time"

[background_vm]
INTERPRETER = "#!/usr/bin/python3"
SCHOST = "localhost"
tasks = ["Background_vm"]

[background_hpc]
SCHOST = "localhost"
tasks = ["creategrib", "Background_hpc", "UnitTest"]
wrapper = "time"

[serial]
SCHOST = "lumi-batch"
NPROC = 1
WRAPPER = "srun"
tasks = ["Gmted", "Soil"]

[serial.BATCH]
NAME = "#SBATCH --job-name=@TASK_NAME@"
ACCOUNT = "#SBATCH -A project_465000527"
WALLTIME = "#SBATCH --time=00:15:00"
PARTITION = "#SBATCH --partition=standard"
NODES = "#SBATCH --nodes=1"
NTASKS = "#SBATCH --ntasks=1"

[serial.ENV]
AAAAOS = "import os"
AAAASYS = "import sys"
AAAMODULE_ENV = "exec(open('/usr/share/lmod/8.3.1/init/env_modules_python.py').read())"
AAMODULE_USE = "module('use', '/scratch/project_462000140/SW/modules')"
AMODULE_ECFLOW = "module('load', 'ecflow/5.9.2')"
BSYS_PATH = "sys.path.insert(0, '/scratch/project_462000140/SW/spack-setup/spack/22.08/0.18.1/ecflow-5.9.2-4eg6sen/lib/python3.9/site-packages/')"
ECF_SSL =  "os.environ['ECF_SSL'] = '1'"
NPROC = 32
MPITASKS_PER_NODE = 8
MPI_TASKS = 32
OMP_NUM_THREADS= "os.environ['OMP_NUM_THREADS'] = '4'"
OMPI_MCA_btl= "os.environ['OMPI_MCA_btl'] = '^vader'"
DR_HOOK_IGNORE_SIGNALS = "os.environ['DR_HOOK_IGNORE_SIGNALS'] = '8'"

[task_exceptions.PgdUpdate]
bindir = "/home/snh02/work/gl_binaries"
binary = "gl_20230823"

[task_exceptions.creategrib]
bindir = "/home/snh02/work/gl_binaries"
binary = "gl_20230823"

[task_exceptions.Gmted.ENV]
MODULE_GDAL= "module('load', 'gdal/3.2.1')"

[task_exceptions.Soil.ENV]
MODULE_GDAL= "module('load', 'gdal/3.2.1')"

[parallel]
SCHOST = "lumi"
tasks = ["Forecast", "e927", "Pgd", "Prep", "c903"]
NPROC = 32
WRAPPER = "srun"
#WRAPPER = "sbatch"
#WRAPPER = "srun -n @NPROC@"

[parallel.BATCH]
NAME = "#SBATCH --job-name=@TASK_NAME@"
ACCOUNT = "#SBATCH -A project_465000527"
PARTITION = "#SBATCH -p debug"
WALLTIME = "#SBATCH --time=00:10:00"

[parallel.ENV]
AAAAOS = "import os"
AAAASYS = "import sys"
AAAMODULE_ENV = "exec(open('/usr/share/lmod/8.3.1/init/env_modules_python.py').read())"
#AAMODULE_USE = "module('use', '/scratch/project_462000140/SW/modules')"
#AMODULE_ECFLOW = "module('load', 'ecflow/5.9.2')"
BSYS_PATH = "sys.path.insert(0, '/scratch/project_462000140/SW/spack-setup/spack/22.08/0.18.1/ecflow-5.9.2-4eg6sen/lib/python3.9/site-packages/')"
ECCODES_SAMPLES_PATH="sys.path.insert(0, '/users/ospaniel/opt/share/eccodes/ifs_samples/grib1_mlgrib2')"
ECCODES_DEFINITION_PATH="sys.path.insert(0, '/users/ospaniel/opt/share/eccodes/definitions/grib2/localConcepts/lfpw:/users/ospaniel/opt/eccodes-2.27.0/share/eccodes/definitions')"

ECF_SSL =  "os.environ['ECF_SSL'] = '-1'"

# Nodes, MPI tasks etc
NNODES="os.environ['NNODES'] = '4'" # $SLURM_JOB_NUM_NODES
MPI_TASKS="os.environ['MPI_TASKS'] = '32'"  # $SLURM_NTASKS
MPITASKS_PER_NODE="os.environ['MPITASKS_PER_NODE'] ='32'" # MPI_TASKS/NNODES
OMP_NUM_THREADS="os.environ['OMP_NUM_THREADS'] = '4'" # CPUS_PER_TASK

# Open-MP business :
# OMP_PLACES looks important for the binding by srun :
OMP_PLACES = "os.environ['OMP_PLACES'] = 'threads'"
OMP_STACKSIZE = "os.environ['OMP_STACKSIZE'] = '4G'"
KMP_STACKSIZE = "os.environ['KMP_STACKSIZE'] = '4G'"
KMP_MONITOR_STACKSIZE = "os.environ['KMP_MONITOR_STACKSIZE'] = '4G'"

# Bitwise reproductibility with MKL :
#MKL_CBWR = "os.environ['MKL_CBWR'] = 'AUTO,STRICT'"
#MKL_NUM_THREADS = "os.environ['MKL_NUM_THREADS'] = '1'"
#MKL_DEBUG_CPU_TYPE = "os.environ['MKL_DEBUG_CPU_TYPE'] = '5'"

# ECMWF prefers "release" than "release_mt" with Intel MPI library :
#I_MPI_LIBRARY_KIND = "os.environ['I_MPI_LIBRARY_KIND'] = 'release'"

# DR_HOOK
DR_HOOK="os.environ['DR_HOOK'] = '1'"
DR_HOOK_IGNORE_SIGNALS="os.environ['DR_HOOK_IGNORE_SIGNALS'] = '8'"
DR_HOOK_SILENT="os.environ['DR_HOOK_SILENT'] = '1'"
DR_HOOK_SHOW_PROCESS_OPTIONS="os.environ['DR_HOOK_SHOW_PROCESS_OPTIONS'] = '0'"

#MPL
#MPL_MBX_SIZE="os.environ['MPL_MBX_SIZE'] = '4096000000'"

# EC PROFILE
EC_PROFILE_HEAP = "os.environ['EC_PROFILE_HEAP'] = '0'"
EC_PROFILE_MEM = "os.environ['EC_PROFILE_MEM'] = '0'"
EC_MPI_ATEXIT = "os.environ['EC_MPI_ATEXIT'] = '0'"
EC_MEMINFO = "os.environ['EC_MEMINFO'] = '0'"

FI_CXI_RX_MATCH_MODE="os.environ['FI_CXI_RX_MATCH_MODE'] = 'hybrid'"

AMODULES = "module('--force', 'purge')"
#AMODULES_LUMI = "module('load', 'LUMI partition/C cpe/23.03 craype-x86-trento PrgEnv-cray cray-mpich cray-libsci partition/G cray-hdf5-parallel cray-netcdf-hdf5parallel rocm')"

#AMODULES_LUMI = "module('load', 'LUMI/23.03  partition/G PrgEnv-cray cpe/23.03 craype-x86-trento cray-mpich cray-libsci cray-fftw cray-python cray-hdf5-parallel cray-netcdf-hdf5parallel buildtools ncurses/6.4-cpeCray-23.03 rocm/5.3.3')"
AMODULES_LUMI = "module('load', 'LUMI/23.03  partition/C PrgEnv-cray cpe/23.03 craype-x86-trento cray-mpich cray-libsci cray-fftw cray-python cray-hdf5-parallel cray-netcdf-hdf5parallel buildtools ncurses/6.4-cpeCray-23.03')"

#MPIAUTO_OPTIONS="os.environ['MPIAUTO_OPTIONS'] = '--use-slurm-mpi --use-slurm-bind --wrap --wrap-stdeo --wrap-stdeo-pack --init-timeout-restart 1'"
#MPIAUTO="os.environ['MPIAUTO'] = '/home/sor/install/mpiauto/mpiauto'"

[task_exceptions.e927]
NPROC = 16

[task_exceptions.e927.BATCH]
NODES = "#SBATCH --nodes=1"
NTASKS = "#SBATCH --ntasks=16"

[task_exceptions.Forecast]
DR_HOOK="os.environ['DR_HOOK'] = '1'"
#DR_HOOK_SILENT="os.environ['DR_HOOK_SILENT'] = '1'"
#DR_HOOK_SHOW_PROCESS_OPTIONS="os.environ['DR_HOOK_SHOW_PROCESS_OPTIONS'] = '0'"


#Configuration for GPU
APL_ALARO = "os.environ['APL_ALARO'] = '1'"
INPART = "os.environ['INPART'] = '1'"
PARALLEL = "os.environ['PARALLEL'] = '1'"

WRAPPER = "ulimit -s unlimited ;  env > env_denis.txt; srun -n @NPROC@ ./select_gpu"
#WRAPPER = "ulimit -s unlimited ;  env > env_denis.txt; srun -n @NPROC@"
[task_exceptions.Forecast.BATCH]
#Denis Settings
AAAAOS = "import os"
DR_HOOK="os.environ['DR_HOOK'] = '1'"

#Environment variables for ALARO PARALLEL
APL_ALARO="os.environ['APL_ALARO'] = '1'"
INPART="os.environ['INPART'] = '1'"
PARALLEL="os.environ['PARALLEL'] = '1'"

#Environment for LUMI GPU
#Allocate 8Gb of heap size on the GPU
CRAY_ACC_MALLOC_HEAPSIZE="os.environ['CRAY_ACC_MALLOC_HEAPSIZE'] = '8000000000'"

#Allow direct GPU to GPU MPI connection s
MPICH_GPU_SUPPORT_ENABLED="os.environ['MPICH_GPU_SUPPORT_ENABLED'] = '1'"

# GPU: bind 8 MPI tasks to certain CPU cores (so that GPUs 0-7 are connected to tasks 0-7)
CPU_BIND="os.environ['CPU_BIND'] = 'mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000'"

#Slurm settings for GPU
NODES = "#SBATCH --nodes=4"
TASKS_PER_NODE = "#SBATCH --ntasks-per-node=8"
CPU = "#SBATCH --cpus-per-task=6"
WALLTIME = "#SBATCH --time=00:30:00"
PARTITION = "#SBATCH --partition=dev-g"
JOBNAME = "#SBATCH --job-name=Alaro_gpu"
ACCOUNT = "#SBATCH --account=project_465000527"
#EXCLUSIVE= "#SBATCH --exclusive"
GPUS_PER_NODE= "#SBATCH --gpus-per-node=8"

#[task_exceptions.Forecast.ENV]
#RTTOV_COEFDIR="/scratch/project_465000527/de_33050_common_data/DEODE/satellite/rtcoef_rttov12/harm_coef/"

[task_exceptions.Pgd]
WRAPPER="srun -n @NPROC@"

[task_exceptions.Pgd.BATCH]
NODES = "#SBATCH --nodes=2"
NTASKS = "#SBATCH --ntasks=32"
WALLTIME = "#SBATCH --time=00:10:00"

[task_exceptions.Prep]
WRAPPER="srun -n @NPROC@"

[task_exceptions.Prep.BATCH]
NODES = "#SBATCH --nodes=1"
NTASKS = "#SBATCH --ntasks=1"
WALLTIME = "#SBATCH --time=00:10:00"

[task_exceptions.c903]
NPROC = 36

[task_exceptions.c903.BATCH]
NODE = "#SBATCH --nodes=12"
NTASKS = "#SBATCH --ntasks=36"
MEM = "#SBATCH --mem=200GB" 
WALLTIME = "#SBATCH --time=00:30:00" 

[task_exceptions.marsprep.BATCH]
NODES = "#SBATCH --nodes=1"
NTASKS = "#SBATCH --ntasks=1"
WALLTIME = "#SBATCH --time=01:00:00"
