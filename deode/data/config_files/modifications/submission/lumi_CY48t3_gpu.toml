[submission.task_exceptions.Forecast]
  bindir = "/pfs/lustrep4/projappl/project_465000527/dhaumont/accord/pack/freeze_gpu_feb_2024/bin"
  WRAPPER = "ulimit -s unlimited; srun -n @NPROC@ ./select_gpu"

[submission.task_exceptions.Forecast.BATCH]
  CPU = "#SBATCH --cpus-per-task=6"
  GPUS_PER_NODE = "#SBATCH --gpus-per-node=8"
  PARTITION = "#SBATCH --partition=dev-g"

[submission.task_exceptions.Forecast.ENV]
  #Environment variables for ALARO PARALLEL
  APL_ALARO = 1
  INPART = 1
  PARALLEL = 1

  #Environment for LUMI GPU
  #Allocate 8Gb of heap size on the GPU
  CRAY_ACC_MALLOC_HEAPSIZE = 8000000000

  #Allow direct GPU to GPU MPI connection
  MPICH_GPU_SUPPORT_ENABLED = 1

  # GPU: bind 8 MPI tasks to certain CPU cores (so that GPUs 0-7 are connected to tasks 0-7)
  CPU_BIND = 'mask_cpu:7e000000000000,7e00000000000000,7e0000,7e000000,7e,7e00,7e00000000,7e0000000000'

  #AMD_LOG_LEVEL=3
  #CRAY_ACC_DEBUG=3
  #ACC_NOTIFY=3
  NOUTPUT = 2

[submission.task_exceptions.Forecast.MODULES]
  02_PARTITION_G = ["load", "partition/G"]
  06_ACCEL = ["load", "craype-accel-amd-gfx90a"]
  07_ROCM = ["load", "rocm"]
  08_MPICH = ["load", "cray-mpich"]
  09_LIBSCI = ["load", "cray-libsci"]
  10_FFTW = ["load", "cray-fftw"]
  11_PYTHON = ["load", "cray-python/3.10.10"]
  12_HDF5 = ["load", " cray-hdf5-parallel"]
  13_NETCDF = ["load", "cray-netcdf-hdf5parallel"]
  14_BUILDTOOLS = ["load", "buildtools"]
  15_NCURSES = ["load", "ncurses/6.4-cpeCray-23.03"]
  16_RIMVYDAS_USE = []
  17_RIMVYDAS_FTN16 = []
  18_RIMVYDAS_ECFLOW = []

