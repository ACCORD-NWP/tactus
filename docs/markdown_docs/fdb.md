# Some notes about using the FDB archive
FDB (Fields DataBase) is a domain-specific object store developed at ECMWF for storing, indexing and retrieving GRIB data. Each GRIB message is stored as a field and indexed trough semantic metadata (i.e. physical variables such as temperature, pressure, â€¦). A set of fields can be retrieved specifying a request using a specific language developed for accessing MARS Archive. Read more about fdb [here](https://fields-database.readthedocs.io/en/latest/).

## Archive data in FDB 
To archive data in FDB three changes have to made in the configuration. Firs we have to get an experiment identifier or `expver`. On the ECMWF HPC ATOS run the following commands (requires the user to be in unix group `ifs`):

```bash
module load pifsenv
getNewId -g d1.on-demand-extremes-dt
```
Note, expver is case-sensitive, so the expver `aaaa` and e.g. `Aaaa` is not the same. Numerical experiment identifiers are to be used for operations only. The next step is to add this `expver` to the config and attach it to your user on the HPC. This is done by adding a rule to `expver_restrictions`. Finally we activate the archiving itself as shown below.
```
[fdb.grib_set]
   expver = "aaaa"
[fdb.expver_restrictions]
  aaaa = "YOUR_USER"
[archiving.FDB.fdb.fpgrib_files]
  active = true
```
where aaaa should be replaced with the expver generated by getNewId and YOUR_USER is the username on the computer where you call FDB.

## Preparing the fdb tool
This is to use the fdb command line tool, if you want to read, write or wipe the data on FDB.

On ATOS:
```
export FDB_HOME=/home/fdbtest
```

On LUMI:
```
module use /appl/local/climatedt/modules
```

## How to list available data
To list available data in fdb, use the fdb list (Here with head -10 to only read the 4 first entries):
```
fdb list class=d1,expver=aaaa | head -10
```
`[OUTPUT]`
```
Listing for request
retrieve,
	class=d1,
	expver=aaaa


{class=d1,dataset=on-demand-extremes-dt,expver=aaaa,stream=oper,date=20250209,time=0000}{type=fc,levtype=sfc,georef=u15rxs}{step=6,param=129}
{class=d1,dataset=on-demand-extremes-dt,expver=aaaa,stream=oper,date=20250209,time=0000}{type=fc,levtype=sfc,georef=u15rxs}{step=6,param=130}
{class=d1,dataset=on-demand-extremes-dt,expver=aaaa,stream=oper,date=20250209,time=0000}{type=fc,levtype=sfc,georef=u15rxs}{step=6,param=134}
{class=d1,dataset=on-demand-extremes-dt,expver=aaaa,stream=oper,date=20250209,time=0000}{type=fc,levtype=sfc,georef=u15rxs}{step=6,param=151}
```

## How to retrieve archived data from FDB:
`fdb read request.mars out.grib`

Where the request.mars file looks something like this (Constructed from last line of output of the fdb list above):
```
retrieve,
	class = d1,
	dataset = on-demand-extremes-dt,
	expver = aaaa,
	stream = oper,
	date = 20250209,
	time = 0000,
	type = fc,
	levtype = sfc,
	georef = u15rxs,
	step = 6,
	param = 151
```

## How to set expver in DEODE Workflow
expver is set in the `[fdb.grib_set]` section of the `archiving.toml` file like this:
```
[fdb.grib_set]
  [...]
  tablesVersion = "32"
  expver = "aaaa"
```
```{note}
The combination of `class`, `dataset`, `expver`, `stream`, `date` and `time` must be unique for each user (user `A` can only archive to fdb folders owned by user `A`). This is fixed by using the expver generator explained above.
```

You also need to enable fdb archiving by setting the `active` variable for the relevant archiving tasks to `true`. Example for `[archiving.hour.fdb.grib2_files]`:

```
[archiving.hour.fdb.grib2_files]
  active = true
  inpath = "@ARCHIVE@"
  pattern = "GRIBPF*"
```

## How to exclude variables from being archived
This is done by adding values to the `archiving.toml` configuration file under the `[fdb.negative_rules]` section:

```
[fdb]

[fdb.negative_rules]
  typeOfLevel = ["unknown", "not_found"]
```
This will exclude all grib-messages where `typeOfLevel` is `unknown` OR `not_found`. The values to filter can either be a single value or list.

```{note}
Currently on ATOS there is a bug where ECCODES will give an error: `ECCODES ERROR   :  Unable to get typeOfLevel as string (Key/value not found)`, but still run as intended.
```


## How to remove data
Use `fdb wipe`

To list out what data will be deleted:
```
fdb wipe class=d1,dataset=on-demand-extremes-dt,expver=aaaa,stream=oper,date=20250209,time=0000
```
And to actually delete it (add the --doit flag):
```
fdb wipe class=d1,dataset=on-demand-extremes-dt,expver=aaaa,stream=oper,date=20250209,time=0000 --doit
```

```{note}
This is a bit buggy on LUMI as the as the archive.lock file created while clearing the fdb-directory is not removed and as such you can not archive again into the same folder (defined by the combination class,dataset,expver,stream,date,time) without removing this archive.lock file.
```


## Notes about georef
We are using a [geohashing algorithm](https://github.com/destination-earth-digital-twins/geohash) with *a* Base32 string-representation of the hash, and Z-order space-filling curves with precision set to 6.
